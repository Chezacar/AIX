### 线性回归的通用化概率解释

------

【参考资料】

PRML  第三章



#### 1. 线性回归的拓展

回归问题的最简单模型是输入变量的线性组合：
$$
y(\boldsymbol{x}, \boldsymbol{w})=w_{0}+w_{1} x_{1}+\ldots+w_{D} x_{D} \tag{1.1}
$$
其中$\boldsymbol{x}=\left(x_{1}, \dots, x_{D}\right)^{T}$。这就是线性回归模型。这个模型的关键性质是它是参数$w_{0}, \dots, w_{D}$的一个线性函数，但同时，它也是输入变量$x_{i}$的一个线性函数，这给模型带来很大的局限性。因此我们这样扩展模型的类别：将输入变量进行非线性映射，然后再建立它们的线性组合，形式为：
$$
y(\boldsymbol{x}, \boldsymbol{w})=w_{0}+\sum_{j=1}^{M-1} w_{j} \phi_{j}(\boldsymbol{x})  \tag{1.2}
$$
其中，$\phi_{j}(\boldsymbol{x})$被称为基函数（basis function），$w_{0}$是偏置。通过把下标$j$的最大值记作$M-1$，这个模型的参数总数为$M$。

通常，定义一个额外的“虚基函数”$\phi_{0}(\boldsymbol{x})=1$，有：
$$
y(\boldsymbol{x}, \boldsymbol{w})=\sum_{j=0}^{M-1} w_{j} \phi_{j}(\boldsymbol{x})=\boldsymbol{w}^{T} \boldsymbol{\phi}(\boldsymbol{x}) \tag{1.3}
$$
其中$w=\left(w_{0}, \dots, w_{M-1}\right)^{T}$且$\phi=\left(\phi_{0}, \dots, \phi_{M-1}\right)^{T}$。

当我们使用基函数时，实际上我们相当于对原始的输入进行了特征变换，新生成的特征就是各基函数的值。

通过使用非线性基函数，我们能够让函数$y(\boldsymbol{x}, \boldsymbol{w})$成为输入向量$\boldsymbol{x}$的一个非线性函数。但是，形如式（1.2）的模型仍被称为线性模型，因为这个函数是$\boldsymbol{w}$的线性函数。

多项式回归就是用基函数拓展的线性回归中的一种。除此以外，还会使用高斯基函数：
$$
\phi_{j}(x)=\exp \left\{-\frac{\left(x-\mu_{j}\right)^{2}}{2 s^{2}}\right\}
$$
或者sigmoid基函数：
$$
\phi_{j}(x)=\sigma\left(\frac{x-\mu_{j}}{s}\right)
$$
其中$\sigma(a)$是sigmoid函数，定义为：
$$
\sigma_{a}=\frac{1}{1+\exp (-a)}
$$


#### 2. 最大似然与MSE

假设目标变量$t$由确定的函数$y(\boldsymbol{x}, \boldsymbol{w})$给出，这个函数被附加了高斯噪声，即：
$$
t=y(\boldsymbol{x}, \boldsymbol{w})+\epsilon \tag{2.1}
$$
其中$\epsilon$是⼀个零均值的⾼斯随机变量，精度（方差的倒数）为$\beta = 1/\sigma^2$。因此我们有：
$$
p(t | \boldsymbol{x}, \boldsymbol{w}, \beta)=\mathcal{N}\left(t | y(\boldsymbol{x}, \boldsymbol{w}), \beta^{-1}\right) \tag{2.2}
$$

现在考虑一个输入数据集$\boldsymbol{X}=\left\{\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{N}\right\}$，对应的目标值为$\boldsymbol{T}=\{t_{1}, \dots, t_{N}\}$。假设这些数据点是独立地从分布（2.2）中抽取的，那么我们可以得到下面的似然函数的表达式：
$$
p(\mathbf{T} | \boldsymbol{X}, \boldsymbol{w}, \beta)=\prod_{n=1}^{N} \mathcal{N}\left(t_{n} | \boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right), \beta^{-1}\right) \tag{2.3}
$$
其中我们使用了式（1.3）。注意在有监督学习问题中，我们不是在寻找模型来对输⼊变量的概率分布建模。因此$x$总会出现在条件变量的位置上。从现在开始，为了保持记号的简洁性，我们在诸如$p(\mathbf{t} | \boldsymbol{x}, \boldsymbol{w}, \beta)$这类的表达式中不显式地写出$x$。

取对数似然函数，并且使用一元高斯分布的标准形式，我们有
$$
\begin{aligned} \ln p(\mathbf{t} | \boldsymbol{w}, \beta) &=\sum_{n=1}^{N} \ln \mathcal{N}\left(t_{n} | \boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right), \beta^{-1}\right) \\ &=\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)-\beta E_{D}(\boldsymbol{w}) \end{aligned} \tag{2.4}
$$
其中平方和误差（MSE）函数的定义为
$$
E_{D}(\boldsymbol{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)\right\}^{2} \tag{2.5}
$$

写出了似然函数后，我们就可以使用最大似然的方法确定$w$和$\beta$。首先求$w$，可以看到公式（2.4）给出的对数似然函数中，只有误差平方和那一项与参数$w$有关，这就是为什么我们选择MSE作为损失函数，其梯度为：
$$
\nabla \ln p(\mathbf{t} | \boldsymbol{w}, \beta)=\beta \sum_{n=1}^{N}\left\{t_{n}-\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)\right\} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)^{T} \tag{2.6}
$$
令其等于0，可得
$$
0=\sum_{n=1}^{N} t_{n} \phi\left(x_{n}\right)^{T}-w^{T}\left(\sum_{n=1}^{N} \phi\left(x_{n}\right) \phi\left(x_{n}\right)^{T}\right)
$$
求解$w$，我们有
$$
\boldsymbol{w}_{M L}=\left(\boldsymbol{\Phi}^{T} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{T} \mathbf{t} \tag{2.7}
$$
这被称为最小二乘问题的规范方程（normal equation ）。这里$\Phi$是一个$N \times M$，其中$N$是样本数量，$M$是基函数映射后的特征维度。$\Phi$被称为设计矩阵（design matrix）：
$$
\mathbf{\Phi}=\left( \begin{array}{cccc}{\phi_{0}\left(\boldsymbol{x}_{1}\right)} & {\phi_{1}\left(\boldsymbol{x}_{1}\right)} & {\cdots} & {\phi_{M-1}\left(\boldsymbol{x}_{1}\right)} \\ {\phi_{0}\left(\boldsymbol{x}_{2}\right)} & {\phi_{1}\left(\boldsymbol{x}_{2}\right)} & {\cdots} & {\boldsymbol{\phi}_{M-1}\left(\boldsymbol{x}_{2}\right)} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\phi_{0}\left(\boldsymbol{x}_{N}\right)} & {\phi_{1}\left(\boldsymbol{x}_{N}\right)} & {\cdots} & {\phi_{M-1}\left(\boldsymbol{x}_{N}\right)}\end{array}\right)
$$
现在，我们可以更进一步地认识偏置系数$w_{0}$。如果我们显式地写出偏置系数，那么误差函数（2.5）就变为
$$
E_{D}(\boldsymbol{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-w_{0}-\sum_{j=1}^{M-1} w_{j} \phi_{j}\left(\boldsymbol{x}_{n}\right)\right\}^{2}
$$
令其关于$w_{0}$的导数为0，解出$w_0$，可得：
$$
w_{0}=\overline{t}-\sum_{j=1}^{M-1} w_{j} \overline{\phi}_{j}
$$
其中我们定义：
$$
\overline{t}=\frac{1}{N} \sum_{n=1}^{N} t_{n}, \quad \overline{\phi}_{j}=\frac{1}{N} \sum_{n=1}^{N} \phi_{j}\left(\boldsymbol{x}_{n}\right)
$$
因此偏置$w_0$补偿了目标值的平均值（在训练集上的）与基函数的值的平均值的加权求和之间的差。

我们也可以关于噪声精度参数$\beta$最大化似然函数（2.4），结果为：
$$
\sigma_{ML}^{2} = \frac{1}{\beta_{M L}}=\frac{1}{N} \sum_{n=1}^{N}\left\{t_{n}-\boldsymbol{w}_{M L}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)\right\}^{2}
$$
因此我们可以看到噪声的方差由目标值在回归函数周围的残留方差（residual variance）给出。



#### 3. L2正则与贝叶斯先验

现在，我们假定参数$w$服从一个精度为$\alpha$，均值为0的高斯先验分布，即：
$$
p(\boldsymbol{w} | \boldsymbol{\alpha})=\mathcal{N}\left(\boldsymbol{w} | \mathbf{0}, \alpha^{-1} \boldsymbol{I}\right) = \left(\frac{\alpha}{2 \pi}\right)^{(M+1) / 2} \exp \left\{-\frac{\alpha}{2} \boldsymbol{w}^{T} \boldsymbol{w}\right\}
$$
于是根据贝叶斯规则，我们可以得到后验概率：
$$
p(w | X, t, \alpha, \beta)=p(t | X, w, \beta) p(w, \alpha) \tag{3.1}
$$
进行最大后验估计，对上面的式子做对数似然，并且去除无关项后，可以得到：
$$
\ln p(\boldsymbol{w} | \mathbf{t})=-\frac{\beta}{2} \sum_{n=1}^{N}\left\{t_{n}-\boldsymbol{w}^{T} \boldsymbol{\phi}\left(\boldsymbol{x}_{n}\right)\right\}^{2}-\frac{\alpha}{2} \boldsymbol{w}^{T} \boldsymbol{w} \tag{3.2}
$$
这告诉我们，后验分布关于$w$的最大化等价于对平方和误差函数加上一个二次正则项进行最小化，其中正则系数$\lambda=\frac{\alpha}{\beta}$。

所以，**当我们对参数作L2正则化时，实际上我们就是要求参数服从于一个高斯的先验分布**。