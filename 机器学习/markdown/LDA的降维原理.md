### LDA的降维原理

***

#### 0. 参考资料

**CSDN** [线性判别分析LDA原理总结](https://www.cnblogs.com/pinard/p/6244265.html)

**CSDN** [【机器学习】LDA线性判别分析](https://blog.csdn.net/u012679707/article/details/80529252)

周志华《机器学习》中对应章节



#### 1. LDA的思想

可以从几何角度来理解LDA降维的原理。LDA是一种监督降维技术，其思想非常简单，给定训练样本，设法将样本投影到一条直线（或者超平面）上，使得**同类样本的的投影点尽可能接近、异类样本的投影点尽可能远离**。

![1549617532665](assets/1549617532665.png)

用数学语言来描述，就是要求不同类别样本的均值相差尽可能大，同类样本的协方差尽可能小。

#### 2. 二分类LDA原理

我们首先从简单的二分类LDA开始，来分析LDA的原理。

假设数据集$D = \left\{ \left( x _ { 1 } , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots , \left( \left( x _ { m } , y _ { m } \right) \right) \right\}$，其中任意样本$x_i$为$n$维向量，$y _ { i } \in \{ 0,1 \}$。我们定义$N _ { j } ( j = 0,1 )$为第$j$类样本的个数，$X _ { j } ( j = 0,1 )$为第$j$类样本的集合，而$\mu _ { j } ( j = 0,1 )$为第$j$类样本的均值向量，定义$\Sigma _ { j } ( j = 0,1 )$为第$j$类样本的协方差矩阵。

$\mu_j$的表达式为：
$$
\mu _ { j } = \frac { 1 } { N _ { j } } \sum _ { x \in X _ { j } } x ( j = 0,1 )
$$
$\Sigma _ { j }$的表达式为：
$$
\Sigma _ { j } = \sum _ { x \in X _ { j } } \left( x - \mu _ { j } \right) \left( x - \mu _ { j } \right) ^ { T } ( j = 0,1 )
$$
若将数据投影到直线$w$上，则两个类别的中心点在直线$w$上的投影为$w ^ { T } \mu _ { 0 }$和$w ^ { T } \mu _ { 1 }$，同类样本点投影的协方差为$w ^ { T } \sum _ { 0 } w$和$w ^ { T } \sum _ { 1 } w$。根据LDA的思想，我们要最大化$\left \| w ^ { T } \mu _ { 0 } - w ^ { T } \mu _ { 1 } \left\| _ { 2 } ^ { 2 }\right. \right.$，同时最小化$w ^ { T } \Sigma _ { 0 } w + w ^ { T } \Sigma _ { 1 } w$。于是得到欲最大化的目标为：
$$
J ( w ) = \frac { \left\| w ^ { T } \mu _ { 0 } - w ^ { T } \mu _ { 1 } \right\| _ { 2 } ^ { 2 } } { w ^ { T } \Sigma _ { 0 } w + w ^ { T } \Sigma _ { 1 } w } = \frac { w ^ { T } \left( \mu _ { 0 } - \mu _ { 1 } \right) \left( \mu _ { 0 } - \mu _ { 1 } \right) ^ { T } w } { w ^ { T } \left( \Sigma _ { 0 } + \Sigma _ { 1 } \right) w }
$$
我们定义类内散度矩阵$S _ { w }$为：
$$
S _ { w } = \Sigma _ { 0 } + \Sigma _ { 1 } = \sum _ { x \in X _ { 0 } } \left( x - \mu _ { 0 } \right) \left( x - \mu _ { 0 } \right) ^ { T } + \sum _ { x \in X _ { 1 } } \left( x - \mu _ { 1 } \right) \left( x - \mu _ { 1 } \right) ^ { T }
$$
同时定义类间散度矩阵$S _ { b }$为：
$$
S _ { b } = \left( \mu _ { 0 } - \mu _ { 1 } \right) \left( \mu _ { 0 } - \mu _ { 1 } \right) ^ { T }
$$
于是优化目标重写为：
$$
J ( w ) = \frac { w ^ { T } S _ { b } w } { w ^ { T } S _ { w } w }
$$
这就是**广义瑞利商**的形式。因此根据广义瑞利商的性质，$J(w)$最大值为矩阵$S _ { w } ^ { - \frac { 1 } { 2 } } S _ { b } S _ { w } ^ { - \frac { 1 } { 2 } }$的最大特征值，即$S _ { w } ^ { - 1 } S _ { b }$的最大特征值。而$w$就是$S _ { w } ^ { - 1 } S _ { b }$的最大特征值对应的特征向量，它和$S _ { w } ^ { - \frac { 1 } { 2 } } S _ { b } S _ { w } ^ { - \frac { 1 } { 2 } }$的特征向量$w ^ { \prime }$满足关系：
$$
w  = S _ { w } ^ { - \frac { 1 } { 2 } } w ^ { \prime }
$$
注意到对于二分类的时候，$S _ { b } w$的方向恒为$\mu _ { 0 } - \mu _ { 1 }$，不妨令$S _ { b } w = \lambda \left( \mu _ { 0 } - \mu _ { 1 } \right)$，将其带入：$\left( S _ { w } ^ { - 1 } S _ { b } \right) w = \lambda w $，可得：
$$
w  = S _ { w } ^ { - 1 } \left( \mu _ { 0 } - \mu _ { 1 } \right)
$$

#### 3. 多分类LDA原理

假设数据集$D = \left\{ \left( x _ { 1 } , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots , \left( \left( x _ { m } , y _ { m } \right) \right) \right\}$，其中任意样本$x_i$为$n$维向量，$y _ { i } \in \left\{ C _ { 1 } , C _ { 2 } , \ldots , C _ { k } \right\}$。我们定义$N _ { j } ( j = 1,2 \ldots k )$为第$j$类样本的个数，$X _ { j } ( j = 1,2 \ldots k )$为第$j$类样本的集合，而$\mu _ { j } ( j = 1,2 \ldots k )$为第$j$类样本的均值向量，定义$\Sigma _ { j } ( j = 1,2 \ldots k )$为第$j$类样本的协方差矩阵。

由于我们是多类向低维投影，则此时投影到的低维空间就不是一条直线，而是一个超平面。假设我们投影到的低维空间的维度为$d$，对应的基向量为$\left( w _ { 1 } , w _ { 2 } , \dots w _ { d } \right)$，基向量组成的矩阵为$W$，它是一个$n \times d$的矩阵。

此时我们的优化目标应该可以变成为:
$$
\frac { W ^ { T } S _ { b } W } { W ^ { T } S _ { w } W }
$$
其中$S _ { b } = \sum _ { j = 1 } ^ { k } N _ { j } \left( \mu _ { j } - \mu \right) \left( \mu _ { j } - \mu \right) ^ { T }​$，$\mu​$为所有样本均值向量。$S _ { w } = \sum _ { j = 1 } ^ { k } S _ { w j } = \sum _ { j = 1 } ^ { k } \sum _ { x \in X _ { j } } \left( x - \mu _ { j } \right) \left( x - \mu _ { j } \right) ^ { T }​$。

这里的问题是，$W ^ { T } S _ { b } W$和$W ^ { T } S _ { w } W$都是矩阵，不是标量，无法作为一个标量函数来优化！也就是说，我们无法直接用二类LDA的优化方法，怎么办呢？一般来说，我们可以用其他的一些替代优化目标来实现。

常见的一个LDA多类优化目标函数定义为：
$$
J ( W ) = \frac { \prod _ { d i a g } W ^ { T } S _ { b } W } { \prod _ { d i a g } W ^ { T } S _ { w } W }
$$
其中$\prod _ { d i a g } A$为$A$的主对角线元素的乘积，$W$为$n \times d$的矩阵。

$J ( W )$的优化过程可以转化为：
$$
J ( W ) = \frac { \prod _ { i = 1 } ^ { d } w _ { i } ^ { T } S _ { b } w _ { i } } { \prod _ { i = 1 } ^ { d } w _ { i } ^ { T } S _ { w } w _ { i } } = \prod _ { i = 1 } ^ { d } \frac { w _ { i } ^ { T } S _ { b } w _ { i } } { w _ { i } ^ { T } S _ { w } w _ { i } }
$$
上式最右边仍为广义瑞利商的形式。因此，最大值是矩阵$S _ { w } ^ { - 1 } S _ { b }$的最大特征值，最大的$d$个值的乘积就是矩阵$S _ { w } ^ { - 1 } S _ { b }$的最大的$d$个特征值的乘积，此时对应的矩阵$W$为这最大的$d$个特征值对应的特征向量张成的矩阵。

需要注意的是，**LDA降维得到的维度$d$的最大值是$k-1$**。

这是因为$S _ { b }$中每个$\mu _ { j } - \mu$的秩为1，$ \left( \mu _ { j } - \mu \right) \left( \mu _ { j } - \mu \right) ^ { T }$的秩也为1（可由Sylvester不等式得到），因此协方差矩阵相加后最大的秩为$k$（矩阵的秩小于等于各个相加矩阵的秩的和），但是由于如果我们知道前$k-1$个$\mu_j$后，最后一个$\mu_k$可以由前$k-1$个$\mu_j$和$\mu$线性表示，因此$S _ { b }$的秩最大为$k-1$，即特征向量最多有$k-1$个。直觉上，也可以理解为，当我们知道了前$k-1$个类别的均值后，也就知道了属于这些类别的具体是哪些样本（因为只有这样才可以算出均值），那么属于剩下最后一个类别的样本就是总样本中剩下的那些样本，从而可以求出最后一个均值。

#### 4. LDA降维算法流程

输入：数据集$D = \left\{ \left( x _ { 1 } , y _ { 1 } \right) , \left( x _ { 2 } , y _ { 2 } \right) , \ldots , \left( \left( x _ { m } , y _ { m } \right) \right) \right\}$，其中任意样本$x_i$为$n$维向量，$y _ { i } \in \left\{ C _ { 1 } , C _ { 2 } , \dots , C _ { k } \right\}$，降维到的维度$d$。

输出：降维后的样本集$D′$。

1）计算类内散度矩阵$S _ { w }$

2）计算类间散度矩阵$S _ { b }$

3）计算矩阵$S _ { w } ^ { - 1 } S _ { b }$

4）计算$S _ { w } ^ { - 1 } S _ { b }$的最大的$d$个特征值和对应的$d$个特征向量$\left( w _ { 1 } , w _ { 2 } , \dots w _ { d } \right)$，得到投影矩阵$W$

5）对样本集中的每一个样本特征$x _ { i }$，转化为新的样本$z _ { i } = W ^ { T } x _ { i }$

6） 得到输出样本集$D ^ { \prime } = \left\{ \left( z _ { 1 } , y _ { 1 } \right) , \left( z _ { 2 } , y _ { 2 } \right) , \ldots , \left( \left( z _ { m } , y _ { m } \right) \right) \right\}$

#### 5. LDA vs PCA

LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。

首先我们看看相同点：

* 两者均可以对数据进行降维。

* 两者在降维时均使用了矩阵特征分解的思想。
* 两者都假设数据符合高斯分布。

我们接着看看不同点：

* LDA是有监督的降维方法，而PCA是无监督的降维方法

* LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。

* LDA除了可以用于降维，还可以用于分类。

* LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。

  因此，LDA在样本分类信息依赖均值而不是方差的时候（即均值差别较大时），比PCA之类的算法较优。

  <img src="assets/1549624671961.png" style="zoom:75%">

  LDA在样本分类信息依赖方差而不是均值的时候（即均值相差不大时），降维效果不好。

  <img src="assets/1549624742544.png" style="zoom:75%">

  

