### 生成模型与判别模型

------

主要参考：

CSDN@[zouxy09](https://me.csdn.net/zouxy09) [生成模型与判别模型](https://blog.csdn.net/zouxy09/article/details/8195017)

#### 1. 生成方法和判别方法

监督学习方法又分生成方法（Generative approach）和判别方法（Discriminative approach），所学到的模型分别称为生成模型（Generative Model）和判别模型（Discriminative Model）。

* **判别方法**：由数据**直接学习决策函数$Y = f ( X )$或者条件概率分布$P ( Y | X )$作为预测的模型**，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别方法包括K近邻、逻辑回归、决策树、感知机、支持向量机等。
* **生成方法**：由数据学习联合概率密度分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型：$P(Y|X)= P(X,Y)/ P(X)$。基本思想是首先建立样本的联合概率概率密度模型$P(X,Y)$，然后再得到后验概率$P(Y|X)$，再利用它进行分类。这整个过程还涉及到对训练数据的概率分布$P(X)$的建模。典型的生成模型包括朴素贝叶斯、混合高斯、隐马尔科夫模型等。

#### 2. 生成模型与判别模型的优缺点

**生成方法的特点**：生成方法学习联合概率密度分布$P(X,Y)$，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。但生成方法估计数据分布$P(X)$时需要使用大量的样本才能保证结果的准确性，对$P(X)​$建模也引入了额外的工作量。

**判别方法的特点**：由于判别方法直接学习条件概率分布$P(Y|X)$，因此不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。事实上，对于分类任务而言，描述数据本身的分布是没有必要的。判别模型面对预测，往往学习的准确率更高，大大简化了学习问题。

另外，**由生成模型可以得到判别模型，但由判别模型得不到生成模型**。

更具体的优缺点总结见下列表格（来自于http://blog.csdn.net/wolenski/article/details/7985426）

![img](assets/1347799026_3378.png)

#### 3. 例子与总结

举个形象的例子，假如你的任务是识别一段话属于汉语、英语、法语中的哪种语言，有两种方法可以达到这个目的：

1、学习每一种语言，如果你花费了大量精力把中英法三种语言都学会了，那么分辨一段话属于哪种语言就是一件很显然的事情，同时你还可以说出新的句子；

2、不去学习每一种语言，只学习这些语言之间的差别（比如字形、发音等的区别），然后再进行判别，那么就要轻松许多。

上述的第一种方法就是生成方法，第二种方法就是判别方法。

最后简单总结一下：

生成算法尝试去找到数据是怎么生成的（产生的），然后再对一个信号进行分类。基于生成假设，哪个类别最有可能产生这个信号，这个信号就属于那个类别。而判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。