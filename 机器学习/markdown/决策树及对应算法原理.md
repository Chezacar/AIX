### 决策树及对应算法原理

------

#### 0. 参考资料

周志华	《机器学习》		清华大学出版社

李航	《统计学习方法》	清华大学出版社

#### 1. 基本模型

一颗决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试或划分。每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。

![1547452144942](assets/1547452144942.png)

**决策树学习本质上是从训练数据集中归纳出一组分类规则**。**决策树的深浅（或者说叶结点的数目）控制了模型复杂度**，深度为1的决策树又被称为**决策树桩（decision stump）**。

#### 2. 特征选择

决策树学习的关键是如何选择最优划分属性。一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”（purity）越来越高。

##### 2.1 信息增益

为便于说明，首先给出熵和条件熵的概念。

设$X$是一个取有限个值的离散随机变量，其概率分布为：
$$
P \left( X = x _ { i } \right) = p _ { i } , \quad i = 1,2 , \cdots , n
$$
则随机变量$X$的熵定义为
$$
H ( X ) = - \sum _ { i = 1 } ^ { n } p _ { i } \log p _ { i }
$$
设有随机变量$( X , Y )$，其联合概率分布为
$$
P \left( X = x _ { i } , Y = y _ { j } \right) = p _ { i j } , \quad i = 1,2 , \cdots , n ; \quad j = 1,2 , \cdots , m
$$
条件熵$H ( Y | X )$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵（conditional entropy）定义为：
$$
H ( Y | X ) = \sum _ { i = 1 } ^ { n } p _ { i } H ( Y | X = x _ { i } )
$$
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。

设训练数据集为$D$，$| D |$表示样本个数。设有$K$个类$C _ { k }$，$k=1,2 , \cdots , K$，$\left| C _ { k } \right|$为属于类$C _ { k }$的样本个数，$\sum _ { k = 1 } ^ { K } \left| C _ { k } \right| = | D |$。设特征$A$有$n$个不同的取值$\left\{ a _ { 1 } , a _ { 2 } , \cdots , a _ { n } \right\}$，根据特征$A$的取值将$D$划分为$n$个子集$D _ { 1 } , D _ { 2 } , \cdots , D _ { n }$，$\left| D _ { i } \right|$为$D _ { i }$的样本个数，$\sum _ { i = 1 } ^ { n } \left| D _ { i } \right| = | D |$。记子集$D _ { i }$中属于类$C _ { k }$的样本的集合为$D _ { i k }$，即$D _ { k } = D _ { i } \cap C _ { k }$，$\left| D _ { i k } \right|$为$D _ { i k }$的样本个数。

于是数据集$D$的经验熵$H ( D )$计算如下：
$$
H ( D ) = - \sum _ { k = 1 } ^ { K } \frac { \left| C _ { k } \right| } { | D | } \log _ { 2 } \frac { \left| C _ { k } \right| } { | D | }
$$
特征$A$对数据集$D$的经验条件熵$H ( D | A )$为：
$$
H ( D | A ) = \sum _ { i = 1 } ^ { n } \frac { \left| D _ { i } \right| } { | D | } H \left( D _ { i } \right) = - \sum _ { i = 1 } ^ { n } \frac { \left| D _ { i } \right| } { |D | } \sum _ { k = 1 } ^ { K } \frac { \left| D _ { i k } \right| } { \left| D _ { i } \right| } \log _ { 2 } \frac { \left| D _ { i k } \right| } { \left| D _ { i } \right| }
$$
**信息增益（information gain）**定义为集合$D$的经验熵$H(D)$与特征$A$给定条件$D$的经验条件熵$H ( D | A )$之差，即：
$$
g ( D , A ) = H ( D ) - H ( D | A )
$$
一般地，熵$H ( Y )$与条件熵$H ( Y | X )$之差称为互信息（mutual information）。**决策树学习中的信息增益等价于训练数据集中类与特征的互信息。**

##### 2.2 信息增益率

信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，可以使用**增益率（gain ratio）**来选择最优划分属性。

增益率定义为：
$$
g _ { R } ( D , A ) = \frac { g ( D , A ) } { \text{IV} ( A ) }
$$
其中，
$$
\mathrm { IV } ( A ) = - \sum _ { i = 1 } ^ { n } \frac { \left| D _ { i } \right| } { | D | } \log _ { 2 } \frac { \left| D _ {i} \right| } { | D | }
$$
称为属性$A$的固有值（intrinsic value）。特征$A$的可能取值数目越多，则$\mathrm { IV } ( A )$的值通常会越大。不过需注意的是，增益率准则对可取值数目较少的属性有所偏好。

#### 3. 决策树的生成

本节介绍经典的决策树生成算法：ID3算法和C4.5算法，这些都是通过**递归**的方式生成决策树。

##### 3.1 ID3算法

ID3算法使用信息增益作为划分准则，算法如下：

**输入**：训练数据集$D$，特征集$A$，阈值$\varepsilon$；

**输出**：决策树$T$

**1**：若$D$中所有实例属于同一类$C _ { k }$，则$T$为单结点树，并将类$C _ { k }$作为该结点的类标记，返回$T$；

**2**：若$A = \varnothing$，则$T$为单结点树，并将$D$中实例数最多的类$C _ { k }$作为该结点的类标记，返回$T$；

**3**：否则，计算$A$中各特征对$D$的信息增益，选择信息增益**最大**的特征$A _ { g }$；

**4**：如果$A _ { g }$的信息增益小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C _ { k }$作为该结点的类标记，返回$T$；

**5**：否则，对$A _ { g }$的每一可能值$a _ { i }$，依$A _ { \mathrm { g } } = a _ { i }$将$D$分割为若干非空子集$D _ { i }$，将$D _ { i }$中实例数最大的类作为标记，构建子结点，有结点及其子结点构成树$T$，返回$T$；

**6**：对第$i$个子结点，以$D _ { i }$为训练集，以$A - \left\{ A _ { g } \right\}$为特征集，递归地调用步骤1~步骤5，得到子树$T _ { i }$，返回$T _ { i }$。

##### 3.2 C4.5算法

C4.5算法使用增益率作为划分准则，其它与ID3算法类似，具体如下：

**输入**：训练数据集$D$，特征集$A$，阈值$\varepsilon$；

**输出**：决策树$T$

**1**：若$D$中所有实例属于同一类$C _ { k }$，则$T$为单结点树，并将类$C _ { k }$作为该结点的类标记，返回$T$；

**2**：若$A = \varnothing$，则$T$为单结点树，并将$D$中实例数最多的类$C _ { k }$作为该结点的类标记，返回$T$；

**3**：否则，计算$A$中各特征对$D$的信息增益，选择信息增益率**最大**的特征$A _ { g }$；

**4**：如果$A _ { g }$的信息增益率小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C _ { k }$作为该结点的类标记，返回$T$；

**5**：否则，对$A _ { g }$的每一可能值$a _ { i }$，依$A _ { \mathrm { g } } = a _ { i }$将$D$分割为若干非空子集$D _ { i }$，将$D _ { i }$中实例数最大的类作为标记，构建子结点，有结点及其子结点构成树$T$，返回$T$；

**6**：对第$i$个子结点，以$D _ { i }$为训练集，以$A - \left\{ A _ { g } \right\}$为特征集，递归地调用步骤1~步骤5，得到子树$T _ { i }$，返回$T _ { i }$。

#### 4. 决策树的剪枝

决策树的生成算法一般都会产生一个过拟合的模型，这是可以使用“剪枝”（pruning）的手段来去掉一些分支，从而降低过拟合风险。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类模型。

《统计学习原理》一书中介绍了一种简单的剪枝算法。决策树的剪枝往往通过极小化决策树整体的损失函数来实现。设树$T$的叶结点个数为$| T |$，$ t $是树$T$的叶结点，该叶结点有$N _ { t }$个样本点，其中$k$类的样本点有$N _ { t k }$个，$k = 1,2 , \cdots , K$，$H _ { t } ( T )$为叶结点$t$上的经验熵，$\alpha \geqslant 0$为参数，则决策树学习的损失函数可以定义为：
$$
C _ { \alpha } ( T ) = \sum _ { t = 1 } ^ { | T | } N _ { t } H _ { t } ( T ) + \alpha | T |\tag{4.1}
$$
其中经验熵为
$$
H _ { t } ( T ) = - \sum _ { k } \frac { N _ { t k } } { N _ { t } } \log \frac { N _ { t k } } { N _ { t } }
$$
我们可以将式（4.1）右端的第一项记为
$$
C ( T ) = \sum _ { t = 1 } ^ { |T| } N _ { t } H _ { t } ( T ) = - \sum _ { t = 1 } ^ { | T ] } \sum _ { k = 1 } ^ { K } N _ { t k } \log \frac { N _ { t k } } { N _ { t } }
$$
这时有
$$
C _ { \alpha } ( T ) = C ( T ) + \alpha | T | \tag{4.2}
$$
式（4.2）中，$C ( T )$表示模型对训练数据的预测误差，$| T |$表示模型复杂度，参数$\alpha \geqslant 0$控制两者之间的影响。

于是，我们可以给出剪枝的算法，如下：

**输入**：生成算法产生的整个树$T$，参数$\alpha$

**输出**：修剪后的子树$T _ { \alpha }$

**1：** 计算每个结点的经验熵

**2：** 递归地从树的叶结点向上回缩

设一组叶结点回缩到其父结点之前与之后的整体树分别为$T _ { B }$与$T _ { A }$，其对应的损失函数分别是$C _ { \alpha } \left( T _ { B } \right)$与$C _ { \alpha } \left( T _ { A } \right)$，如果
$$
C _ { a } \left( T _ { A } \right) \leqslant C _ { \alpha } \left( T _ { B } \right)
$$
则进行剪枝，即将父结点变为新的叶结点。

**3**：返回步骤2，直至不能继续为止，得到损失函数最小的子树$T_{\alpha}$。

#### 5. CART算法

CART算法的全称是分类与回归树（classification and regression tree，CART）。**CART假设决策树是二叉树**，内部结点特征的取值为“是”或“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。

##### 5.1 回归树生成

一个回归树对应着特征空间的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为$M$个单元$R _ { 1 } , R _ { 2 } , \cdots , R _ { M }$，并且在每个单元$R _ { m }$上有一个固定的输出值$c _ { m }$，于是回归树模型可表示为
$$
f ( x ) = \sum _ { m = 1 } ^ { M } c _ { m } I \left( x \in R _ { m } \right)
$$
当输入空间的划分确定时，可以用平方误差$\sum _ { x _ { i } \in R _ { n } } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$来表示回归树对于训练数据的预测误差。易知，单元$R _ { m }$上的$C _ { m }$的最优值$\hat { c } _ { m }$是$R _ { m }$上的所有输入实例$x _ { i }$对应的输出$y _ { i }$的均值，即
$$
\hat { c } _ { m } = \operatorname { avg } \left( y _ { i } | x _ { i } \in R _ { m } \right)
$$
剩下的问题就是怎么对输入空间进行划分。这里采用启发式算法，选择第$j$个变量$x ^ { ( j ) }$和它的取值$s$，作为切分变量和切分点，并定义两个区域：
$$
R _ { 1 } ( j , s ) = \{ x | x ^ { ( j ) } \leqslant s \} \quad \text { & } \quad R _ { 2 } ( j , s ) = \{ x | x ^ { ( j ) } > s \}
$$
然后寻找最优切分变量$j$和最优切分点$s$。具体地，求解
$$
\min _ { j , s } \left[ \min _ { c _ { 1 } } \sum _ { x _ { i } \in R _ { 1 } ( j , s ) } \left( y _ { i } - c _ { 1 } \right) ^ { 2 } + \min _ { c _ { 2 } } \sum _ { x \in R _ { 2 } ( j , s ) } \left( y _ { i } - c _ { 2 } \right) ^ { 2 } \right]
$$
对固定输入变量$j$可以找到最优切分点$s$。
$$
\hat { c } _ { 1 } = \operatorname { avg } \left( y _ { i } | x _ { i } \in R _ { 1 } ( j , s ) \right) \quad \& \quad  \hat { c } _ { 2 } = \operatorname { avg } \left( y _ { i } | x _ { i } \in R _ { 2 } ( j , s ) \right)
$$
遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j,s)$。依此将输入空间划分为两个区域。接着，对每个子区域重复上述过程，直到满足停止条件为止，这样就生成了一颗最小二乘回归树。

##### 5.2 分类树的生成

分类树用**基尼指数（Gini index）**选择最优特征，同时决定该特征的最优二值切分点。

分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p _ { k }$，则概率分布的基尼指数定义为
$$
\operatorname { Gini } ( p ) = \sum _ { k = 1 } ^ { K } p _ { k } \left( 1 - p _ { k } \right) = 1 - \sum _ { k = 1 } ^ { K } p _ { k } ^ { 2 }
$$
对于给定的样本集合$D$，其基尼系数为
$$
\operatorname { Gini } ( D ) = 1 - \sum _ { k = 1 } ^ { K } \left( \frac { \left| C _ { k } \right| } { | D | } \right) ^ { 2 }
$$
这里，$C _ { k }$是$D$中属于第$k$类的样本子集，$K$是类的个数。

如果样本集合$D$个根据特征$A$是否取某一可能值$a$被分割为$D_{1}$和$D_{2}$两部分，即
$$
D _ { 1 } = \{ ( x , y ) \in D | A ( x ) = a \} , \quad D _ { 2 } = D - D _ { 1 }
$$
则在特征$A$的条件下，集合$D$的基尼指数定义为
$$
\operatorname { Gini } ( D , A ) = \frac { \left| D _ { 1 } \right| } { | D | } \operatorname { Gini } \left( D _ { 1 } \right) + \frac { \left| D _ { 2 } \right| } { | D | } \operatorname { Gini } \left( D _ { 2 } \right)
$$
基尼指数$\operatorname { Gini } ( D )$表示集合$D$的不确定性，基尼指数$\operatorname { Gini } ( D , A )$表示经$A = a$分割后集合$D$的不确定性。基尼指数越大，样本集合的不确定性也就越大。

在划分时，对于每个特征$A$和它们的每个可能取值$a$都计算基尼指数，选择基尼指数最小的特征$A$对应的$a$。

一般来说，算法的停止条件有三：1）结点中样本个数小于某个阈值；2）样本集的基尼指数小于某个阈值（样本基本属于同一类）；3）没有更多特征。

##### 5.3 CART剪枝

CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0​$底端开始不断剪枝，直到$T_0​$的根结点，形成一个子树序列$\left\{ T _ { 0 } , T _ { 1 } , \cdots , T _ { n } \right\}​$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选出最优子树。

在剪枝的过程中，子树的损失函数为：
$$
C _ { \alpha } ( T ) = C ( T ) + \alpha | T |
$$
其中，$T$为任意子树，$C ( T )$为对训练数据的预测误差（如基尼指数），$| T |$为子树中的叶结点个数，$\alpha \geqslant 0$为参数，$C _ { \alpha } ( T )$为参数是$\alpha$时的子树$T$的整体损失。参数$a$权衡训练数据的拟合程度与模型的复杂度。

具体地，从整体树$T _ { 0 }$开始剪枝。对$T _ { 0 }$的任意内部结点$t$，以$t$为单结点树的损失函数是
$$
C _ { \alpha } ( t ) = C ( t ) + \alpha
$$
以$t$为根结点的子树$T _ { t }$的损失函数是
$$
C _ { \alpha } \left( T _ { t } \right) = C \left( T _ { t } \right) + \alpha \left| T _ { t } \right|
$$
当$\alpha = 0$及$\alpha$充分小时，由不等式
$$
C _ { \alpha } \left( T _ { t } \right) < C _ { \alpha } ( t )
$$
当$\alpha$增大时，在某一$\alpha$有
$$
C _ { \alpha } \left( T _ { t } \right) = C _ { \alpha } ( t )
$$
此时，有
$$
\alpha = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }
$$
并且$T _ { t }$与$t$有相同的损失函数值，而$t$的结点更少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。

为此，对$T_0$中每一内部结点$t$，计算
$$
g ( t ) = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }
$$
它表示剪枝后整体损失函数减少的程度。

于是，CART剪枝算法如下：

**输入**：CART算法生成的决策树$T_0$；

**输出**：最优决策树$T _ { \alpha }$

**1**：设$k=0$，$T = T _ { 0 }$

**2**：设$\alpha = + \infty$

**3**：自上而下地对各内部结点$t$计算$C \left( T _ { t } \right)$，$\left| T _ { t } \right|$以及
$$
g ( t ) = \frac { C ( t ) - C \left( T _ { t } \right) } { \left| T _ { t } \right| - 1 }
$$

$$
\alpha = \min ( \alpha , g ( t ) )
$$

这里，$T _ { t }$表示以$t$为根结点的子树，$C \left( T _ { t } \right)$是对训练数据的预测误差，$\left| T _ { t } \right|$是$T _ { t }$的叶结点个数。

**4**：自上而下地访问内部结点$t$，如果有$g ( t ) = \alpha$，进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$。

**5**：设$k=k+1$，$\alpha _ { k } = \alpha$，$T _ { k } = T$。

**6**：如果$T$不是由根结点单独构成的树，则回到步骤4。

**7**：采用交叉验证法在子树序列$T _ { 0 } , T _ { 1 } , \cdots , T _ { n }$中选取最优子树$T _ { \alpha }$。